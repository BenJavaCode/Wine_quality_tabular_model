{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wine_quality.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFT1asw6YRSw"
      },
      "source": [
        "#For this project i will try to rank the wines in the dataset obtained [here] in  one of three categories bad, descent, good. the dataset consist of 12\n",
        "#colums, where the 12'th column is the quality, that i will use as labels."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar342WToZL7V"
      },
      "source": [
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "  \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler  \n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hp4Irx-WZemi",
        "outputId": "1194bc7c-b387-48f6-d88b-98ec48a991d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysa98TsLZp-4"
      },
      "source": [
        "path = '/content/gdrive/My Drive/wine_data/winequality-white.csv'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "T5jVIL76ZyXk",
        "outputId": "5140ff6e-6316-4a40-957b-d3f6085a7412"
      },
      "source": [
        "file_out = pd.read_csv(path, delimiter=';')\n",
        "file_out.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.36</td>\n",
              "      <td>20.7</td>\n",
              "      <td>0.045</td>\n",
              "      <td>45.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>1.0010</td>\n",
              "      <td>3.00</td>\n",
              "      <td>0.45</td>\n",
              "      <td>8.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.049</td>\n",
              "      <td>14.0</td>\n",
              "      <td>132.0</td>\n",
              "      <td>0.9940</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.49</td>\n",
              "      <td>9.5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.1</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.40</td>\n",
              "      <td>6.9</td>\n",
              "      <td>0.050</td>\n",
              "      <td>30.0</td>\n",
              "      <td>97.0</td>\n",
              "      <td>0.9951</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.44</td>\n",
              "      <td>10.1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.2</td>\n",
              "      <td>0.23</td>\n",
              "      <td>0.32</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.058</td>\n",
              "      <td>47.0</td>\n",
              "      <td>186.0</td>\n",
              "      <td>0.9956</td>\n",
              "      <td>3.19</td>\n",
              "      <td>0.40</td>\n",
              "      <td>9.9</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.0              0.27         0.36  ...       0.45      8.8        6\n",
              "1            6.3              0.30         0.34  ...       0.49      9.5        6\n",
              "2            8.1              0.28         0.40  ...       0.44     10.1        6\n",
              "3            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "4            7.2              0.23         0.32  ...       0.40      9.9        6\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "jz6b7pMfZzgc",
        "outputId": "6574784d-6615-4269-edab-07708f1dccc9"
      },
      "source": [
        "sns.countplot(x = 'quality', data=file_out)\n",
        "# the distribution of feature labels are very uneven, which might be a problem"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fd02ca975c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0wNcoyoZ6iN"
      },
      "source": [
        "#convert labels into range 0,1,2 bad, descent, good"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NumYuhyoaOPq"
      },
      "source": [
        "for i in range(len(file_out)):\n",
        "  if file_out.iloc[i, 11] <= 5:\n",
        "    file_out.iloc[i, 11] = 0\n",
        "  elif file_out.iloc[i, 11] < 7:\n",
        "    file_out.iloc[i, 11] = 1\n",
        "  else:\n",
        "    file_out.iloc[i, 11] = 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqY1PY1ed2jf"
      },
      "source": [
        "data = file_out.iloc[:,: -1]\n",
        "labels = file_out.iloc[:, 11]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HofKb5eObo_b"
      },
      "source": [
        "# because the is a imbalance of classes, we use stratity to make sure that we have a balanced train and val set\n",
        "data_train, data_val, labels_train, labels_val = train_test_split(data, labels, test_size=0.2, stratify=labels)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkGw-zvdgL-U",
        "outputId": "b53e5158-78df-49ff-8fd3-4ca8d3557e83"
      },
      "source": [
        "labels_train"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3902    1\n",
              "3800    0\n",
              "757     1\n",
              "4807    1\n",
              "4468    1\n",
              "       ..\n",
              "3754    2\n",
              "1766    0\n",
              "1603    2\n",
              "3766    0\n",
              "4368    1\n",
              "Name: quality, Length: 3918, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGCthNzOfZIE"
      },
      "source": [
        "#normalizing the datasets between 0-1\n",
        "scaler = MinMaxScaler()\n",
        "data_train = scaler.fit_transform(data_train)\n",
        "#to avoid data-leakage, we scale the val set with the same parameters as the train set\n",
        "data_val = scaler.transform(data_val)\n",
        "\n",
        "\n",
        "#transforming to numpy arrays for further proccessing\n",
        "data_train, labels_train = np.array(data_train), np.array(labels_train)\n",
        "data_val, labels_val = np.array(data_val), np.array(labels_val)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku_qGEt9hrt1"
      },
      "source": [
        "#to make sure the distributaion is even we count the classes and plot each dataset\n",
        "\n",
        "def get_distr(inp):\n",
        "    count = {\n",
        "        \"bad\": 0,\n",
        "        \"descent\": 0,\n",
        "        \"good\": 0,\n",
        "    }\n",
        "    \n",
        "    for i in inp:\n",
        "        if i == 0: \n",
        "            count['bad'] += 1\n",
        "        elif i == 1: \n",
        "            count['descent'] += 1\n",
        "        else:\n",
        "          count['good'] += 1\n",
        "            \n",
        "    return count"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "H9uLOc22h7xt",
        "outputId": "e7288fb7-8622-4fec-c4cd-78a23a5c733d"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
        "# train labels\n",
        "sns.barplot(data = pd.DataFrame.from_dict([get_distr(labels_train)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[0]).set_title('distr of classes in train')\n",
        "# val labels\n",
        "sns.barplot(data = pd.DataFrame.from_dict([get_distr(labels_val)]).melt(), x = \"variable\", y=\"value\", hue=\"variable\",  ax=axes[1]).set_title('distr of classes in val')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'distr of classes in val')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIgrTUDUjsWQ"
      },
      "source": [
        "#datasets for dataloader\n",
        "class WineDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        return self.data[i], self.labels[i]\n",
        "        \n",
        "    def __len__ (self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "train_dset = WineDataset(torch.from_numpy(data_train).float(), torch.from_numpy(labels_train).long())\n",
        "val_dset = WineDataset(torch.from_numpy(data_val).float(), torch.from_numpy(labels_val).long())\n",
        "\n",
        "#making a dict, from the datasets for neater loop\n",
        "datasets_dict= {'train' : train_dset, 'val' : val_dset }\n",
        "dataset_sizes = {x: len(datasets_dict[x]) for x in ['train', 'val']}\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0bBoeTHncCd",
        "outputId": "a6784a56-17cc-40ba-b3b6-a65bbd23d0d5"
      },
      "source": [
        "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
        "print(f\"Training on device {device}.\")\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(datasets_dict[x], batch_size=20, shuffle=True, num_workers=2, pin_memory=use_cuda) for x in ['train', 'val']}\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on device cuda.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-TS5c9vntnJ"
      },
      "source": [
        "class MultiClassNet(nn.Module):\n",
        "    def __init__(self, features, classes):\n",
        "        super(MultiClassNet, self).__init__()\n",
        "        \n",
        "        self.l1 = nn.Linear(features, 1024)\n",
        "        self.l2 = nn.Linear(1024, 256)\n",
        "        self.l3 = nn.Linear(256, 64)\n",
        "        self.l4 = nn.Linear(64, 16)\n",
        "        self.outL = nn.Linear(16, classes)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.bN1 = nn.BatchNorm1d(1024)\n",
        "        self.bN2 = nn.BatchNorm1d(256)\n",
        "        self.bN3 = nn.BatchNorm1d(64)\n",
        "        self.bN4 = nn.BatchNorm1d(16)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        inp = self.l1(inp)\n",
        "        inp = self.bN1(inp)\n",
        "        inp = self.relu(inp)\n",
        "\n",
        "        inp = self.l2(inp)\n",
        "        inp = self.bN2(inp)\n",
        "        inp = self.relu(inp)\n",
        "\n",
        "        inp = self.l3(inp)\n",
        "        inp = self.bN3(inp)\n",
        "        inp = self.relu(inp)\n",
        "        inp = self.dropout(inp)\n",
        "\n",
        "        inp = self.l4(inp)\n",
        "        inp = self.bN4(inp)\n",
        "        inp = self.relu(inp)\n",
        "        \n",
        "        inp = self.outL(inp)\n",
        "        \n",
        "        return inp"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIPROUFIoKpx",
        "outputId": "da176ddb-dcc6-49ab-aa66-e3041ecda823"
      },
      "source": [
        "# I used nn.CrossEntropyLoss because this is a multiclass classification problem. \n",
        "model = MulticlassClassification(features = 11, classes =3)\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
        "\n",
        "scheduler = lr_scheduler.StepLR( # each 7 steps decay lr by a factor of 0.7\n",
        "    optimizer, step_size=50, gamma= 0.8\n",
        ")\n",
        "print(model)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MulticlassClassification(\n",
            "  (layer_1): Linear(in_features=11, out_features=512, bias=True)\n",
            "  (layer_2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (layer_out): Linear(in_features=128, out_features=3, bias=True)\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4srs_u8o6JJ"
      },
      "source": [
        "def train_model(model, loss_fn, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train() \n",
        "            else:\n",
        "                model.eval()  \n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "               #forward pass\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) # inp, dim\n",
        "                    loss = loss_fn(outputs, labels)\n",
        "\n",
        "                    # backwards pass\n",
        "                    if phase == 'train':\n",
        "                        loss.backward() # calculate gradiens\n",
        "                        optimizer.step() # update params\n",
        "\n",
        "                #stats\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print('\\n')\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load weights of best model\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKzUoj_DpedL",
        "outputId": "eeb4b1eb-c26a-46a8-c0ba-1f232e9523d2"
      },
      "source": [
        "model_hybrid = train_model(\n",
        "    model = model, loss_fn = loss_fn, optimizer = optimizer, scheduler = scheduler, num_epochs=400\n",
        ")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/399\n",
            "----------\n",
            "train Loss: 0.6102 Acc: 0.7284\n",
            "val Loss: 0.7133 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 1/399\n",
            "----------\n",
            "train Loss: 0.6000 Acc: 0.7356\n",
            "val Loss: 0.7278 Acc: 0.6745\n",
            "\n",
            "\n",
            "Epoch 2/399\n",
            "----------\n",
            "train Loss: 0.5920 Acc: 0.7346\n",
            "val Loss: 0.7238 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 3/399\n",
            "----------\n",
            "train Loss: 0.5777 Acc: 0.7514\n",
            "val Loss: 0.7344 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 4/399\n",
            "----------\n",
            "train Loss: 0.5907 Acc: 0.7284\n",
            "val Loss: 0.7248 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 5/399\n",
            "----------\n",
            "train Loss: 0.5907 Acc: 0.7397\n",
            "val Loss: 0.7141 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 6/399\n",
            "----------\n",
            "train Loss: 0.5967 Acc: 0.7266\n",
            "val Loss: 0.7456 Acc: 0.6653\n",
            "\n",
            "\n",
            "Epoch 7/399\n",
            "----------\n",
            "train Loss: 0.5839 Acc: 0.7412\n",
            "val Loss: 0.7301 Acc: 0.6663\n",
            "\n",
            "\n",
            "Epoch 8/399\n",
            "----------\n",
            "train Loss: 0.5740 Acc: 0.7437\n",
            "val Loss: 0.7182 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 9/399\n",
            "----------\n",
            "train Loss: 0.5830 Acc: 0.7407\n",
            "val Loss: 0.7143 Acc: 0.6735\n",
            "\n",
            "\n",
            "Epoch 10/399\n",
            "----------\n",
            "train Loss: 0.5943 Acc: 0.7340\n",
            "val Loss: 0.7154 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 11/399\n",
            "----------\n",
            "train Loss: 0.5779 Acc: 0.7407\n",
            "val Loss: 0.7264 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 12/399\n",
            "----------\n",
            "train Loss: 0.5786 Acc: 0.7397\n",
            "val Loss: 0.7342 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 13/399\n",
            "----------\n",
            "train Loss: 0.5854 Acc: 0.7328\n",
            "val Loss: 0.7177 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 14/399\n",
            "----------\n",
            "train Loss: 0.6048 Acc: 0.7287\n",
            "val Loss: 0.7263 Acc: 0.6490\n",
            "\n",
            "\n",
            "Epoch 15/399\n",
            "----------\n",
            "train Loss: 0.5780 Acc: 0.7443\n",
            "val Loss: 0.7357 Acc: 0.6602\n",
            "\n",
            "\n",
            "Epoch 16/399\n",
            "----------\n",
            "train Loss: 0.5836 Acc: 0.7374\n",
            "val Loss: 0.7418 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 17/399\n",
            "----------\n",
            "train Loss: 0.5700 Acc: 0.7453\n",
            "val Loss: 0.7380 Acc: 0.6684\n",
            "\n",
            "\n",
            "Epoch 18/399\n",
            "----------\n",
            "train Loss: 0.5863 Acc: 0.7435\n",
            "val Loss: 0.7222 Acc: 0.6684\n",
            "\n",
            "\n",
            "Epoch 19/399\n",
            "----------\n",
            "train Loss: 0.5992 Acc: 0.7284\n",
            "val Loss: 0.7206 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 20/399\n",
            "----------\n",
            "train Loss: 0.5851 Acc: 0.7478\n",
            "val Loss: 0.7239 Acc: 0.6745\n",
            "\n",
            "\n",
            "Epoch 21/399\n",
            "----------\n",
            "train Loss: 0.5918 Acc: 0.7310\n",
            "val Loss: 0.7231 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 22/399\n",
            "----------\n",
            "train Loss: 0.5984 Acc: 0.7346\n",
            "val Loss: 0.7212 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 23/399\n",
            "----------\n",
            "train Loss: 0.5860 Acc: 0.7323\n",
            "val Loss: 0.7212 Acc: 0.6796\n",
            "\n",
            "\n",
            "Epoch 24/399\n",
            "----------\n",
            "train Loss: 0.5940 Acc: 0.7343\n",
            "val Loss: 0.7122 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 25/399\n",
            "----------\n",
            "train Loss: 0.6041 Acc: 0.7277\n",
            "val Loss: 0.7212 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 26/399\n",
            "----------\n",
            "train Loss: 0.6007 Acc: 0.7346\n",
            "val Loss: 0.7292 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 27/399\n",
            "----------\n",
            "train Loss: 0.5955 Acc: 0.7343\n",
            "val Loss: 0.7330 Acc: 0.6724\n",
            "\n",
            "\n",
            "Epoch 28/399\n",
            "----------\n",
            "train Loss: 0.5859 Acc: 0.7414\n",
            "val Loss: 0.7212 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 29/399\n",
            "----------\n",
            "train Loss: 0.5948 Acc: 0.7264\n",
            "val Loss: 0.7315 Acc: 0.6673\n",
            "\n",
            "\n",
            "Epoch 30/399\n",
            "----------\n",
            "train Loss: 0.5978 Acc: 0.7374\n",
            "val Loss: 0.7218 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 31/399\n",
            "----------\n",
            "train Loss: 0.5971 Acc: 0.7358\n",
            "val Loss: 0.7207 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 32/399\n",
            "----------\n",
            "train Loss: 0.5895 Acc: 0.7328\n",
            "val Loss: 0.7380 Acc: 0.6714\n",
            "\n",
            "\n",
            "Epoch 33/399\n",
            "----------\n",
            "train Loss: 0.5940 Acc: 0.7407\n",
            "val Loss: 0.7423 Acc: 0.6704\n",
            "\n",
            "\n",
            "Epoch 34/399\n",
            "----------\n",
            "train Loss: 0.5961 Acc: 0.7404\n",
            "val Loss: 0.7374 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 35/399\n",
            "----------\n",
            "train Loss: 0.5930 Acc: 0.7353\n",
            "val Loss: 0.7195 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 36/399\n",
            "----------\n",
            "train Loss: 0.5827 Acc: 0.7366\n",
            "val Loss: 0.7383 Acc: 0.6684\n",
            "\n",
            "\n",
            "Epoch 37/399\n",
            "----------\n",
            "train Loss: 0.5885 Acc: 0.7330\n",
            "val Loss: 0.7098 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 38/399\n",
            "----------\n",
            "train Loss: 0.5795 Acc: 0.7356\n",
            "val Loss: 0.7127 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 39/399\n",
            "----------\n",
            "train Loss: 0.5884 Acc: 0.7417\n",
            "val Loss: 0.7227 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 40/399\n",
            "----------\n",
            "train Loss: 0.5908 Acc: 0.7371\n",
            "val Loss: 0.7228 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 41/399\n",
            "----------\n",
            "train Loss: 0.5793 Acc: 0.7458\n",
            "val Loss: 0.7182 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 42/399\n",
            "----------\n",
            "train Loss: 0.5877 Acc: 0.7366\n",
            "val Loss: 0.7125 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 43/399\n",
            "----------\n",
            "train Loss: 0.5903 Acc: 0.7330\n",
            "val Loss: 0.7200 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 44/399\n",
            "----------\n",
            "train Loss: 0.5878 Acc: 0.7328\n",
            "val Loss: 0.7299 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 45/399\n",
            "----------\n",
            "train Loss: 0.5729 Acc: 0.7481\n",
            "val Loss: 0.7269 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 46/399\n",
            "----------\n",
            "train Loss: 0.5816 Acc: 0.7445\n",
            "val Loss: 0.7154 Acc: 0.7051\n",
            "\n",
            "\n",
            "Epoch 47/399\n",
            "----------\n",
            "train Loss: 0.5735 Acc: 0.7483\n",
            "val Loss: 0.7422 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 48/399\n",
            "----------\n",
            "train Loss: 0.5830 Acc: 0.7422\n",
            "val Loss: 0.7366 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 49/399\n",
            "----------\n",
            "train Loss: 0.5747 Acc: 0.7425\n",
            "val Loss: 0.7213 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 50/399\n",
            "----------\n",
            "train Loss: 0.5623 Acc: 0.7529\n",
            "val Loss: 0.7222 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 51/399\n",
            "----------\n",
            "train Loss: 0.5776 Acc: 0.7397\n",
            "val Loss: 0.7103 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 52/399\n",
            "----------\n",
            "train Loss: 0.5605 Acc: 0.7514\n",
            "val Loss: 0.7187 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 53/399\n",
            "----------\n",
            "train Loss: 0.5743 Acc: 0.7455\n",
            "val Loss: 0.7136 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 54/399\n",
            "----------\n",
            "train Loss: 0.5781 Acc: 0.7414\n",
            "val Loss: 0.7420 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 55/399\n",
            "----------\n",
            "train Loss: 0.5717 Acc: 0.7499\n",
            "val Loss: 0.7327 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 56/399\n",
            "----------\n",
            "train Loss: 0.5759 Acc: 0.7420\n",
            "val Loss: 0.7324 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 57/399\n",
            "----------\n",
            "train Loss: 0.5664 Acc: 0.7420\n",
            "val Loss: 0.7179 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 58/399\n",
            "----------\n",
            "train Loss: 0.5815 Acc: 0.7450\n",
            "val Loss: 0.7241 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 59/399\n",
            "----------\n",
            "train Loss: 0.5758 Acc: 0.7443\n",
            "val Loss: 0.7227 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 60/399\n",
            "----------\n",
            "train Loss: 0.5729 Acc: 0.7499\n",
            "val Loss: 0.7391 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 61/399\n",
            "----------\n",
            "train Loss: 0.5627 Acc: 0.7471\n",
            "val Loss: 0.7313 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 62/399\n",
            "----------\n",
            "train Loss: 0.5910 Acc: 0.7307\n",
            "val Loss: 0.7321 Acc: 0.6643\n",
            "\n",
            "\n",
            "Epoch 63/399\n",
            "----------\n",
            "train Loss: 0.5788 Acc: 0.7422\n",
            "val Loss: 0.7230 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 64/399\n",
            "----------\n",
            "train Loss: 0.5783 Acc: 0.7432\n",
            "val Loss: 0.7327 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 65/399\n",
            "----------\n",
            "train Loss: 0.5777 Acc: 0.7386\n",
            "val Loss: 0.7429 Acc: 0.6745\n",
            "\n",
            "\n",
            "Epoch 66/399\n",
            "----------\n",
            "train Loss: 0.5660 Acc: 0.7527\n",
            "val Loss: 0.7296 Acc: 0.6724\n",
            "\n",
            "\n",
            "Epoch 67/399\n",
            "----------\n",
            "train Loss: 0.5678 Acc: 0.7522\n",
            "val Loss: 0.7208 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 68/399\n",
            "----------\n",
            "train Loss: 0.5747 Acc: 0.7524\n",
            "val Loss: 0.7375 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 69/399\n",
            "----------\n",
            "train Loss: 0.5771 Acc: 0.7389\n",
            "val Loss: 0.7307 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 70/399\n",
            "----------\n",
            "train Loss: 0.6052 Acc: 0.7157\n",
            "val Loss: 0.7391 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 71/399\n",
            "----------\n",
            "train Loss: 0.5765 Acc: 0.7420\n",
            "val Loss: 0.7455 Acc: 0.6684\n",
            "\n",
            "\n",
            "Epoch 72/399\n",
            "----------\n",
            "train Loss: 0.5752 Acc: 0.7471\n",
            "val Loss: 0.7205 Acc: 0.7061\n",
            "\n",
            "\n",
            "Epoch 73/399\n",
            "----------\n",
            "train Loss: 0.5668 Acc: 0.7501\n",
            "val Loss: 0.7309 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 74/399\n",
            "----------\n",
            "train Loss: 0.5742 Acc: 0.7458\n",
            "val Loss: 0.7426 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 75/399\n",
            "----------\n",
            "train Loss: 0.5805 Acc: 0.7404\n",
            "val Loss: 0.7510 Acc: 0.6745\n",
            "\n",
            "\n",
            "Epoch 76/399\n",
            "----------\n",
            "train Loss: 0.5674 Acc: 0.7412\n",
            "val Loss: 0.7372 Acc: 0.6643\n",
            "\n",
            "\n",
            "Epoch 77/399\n",
            "----------\n",
            "train Loss: 0.5722 Acc: 0.7509\n",
            "val Loss: 0.7440 Acc: 0.6704\n",
            "\n",
            "\n",
            "Epoch 78/399\n",
            "----------\n",
            "train Loss: 0.5839 Acc: 0.7440\n",
            "val Loss: 0.7405 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 79/399\n",
            "----------\n",
            "train Loss: 0.5905 Acc: 0.7343\n",
            "val Loss: 0.7377 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 80/399\n",
            "----------\n",
            "train Loss: 0.5845 Acc: 0.7394\n",
            "val Loss: 0.7426 Acc: 0.6663\n",
            "\n",
            "\n",
            "Epoch 81/399\n",
            "----------\n",
            "train Loss: 0.5749 Acc: 0.7414\n",
            "val Loss: 0.7425 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 82/399\n",
            "----------\n",
            "train Loss: 0.5771 Acc: 0.7486\n",
            "val Loss: 0.7311 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 83/399\n",
            "----------\n",
            "train Loss: 0.5798 Acc: 0.7437\n",
            "val Loss: 0.7230 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 84/399\n",
            "----------\n",
            "train Loss: 0.5602 Acc: 0.7552\n",
            "val Loss: 0.7215 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 85/399\n",
            "----------\n",
            "train Loss: 0.5830 Acc: 0.7425\n",
            "val Loss: 0.7308 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 86/399\n",
            "----------\n",
            "train Loss: 0.5775 Acc: 0.7407\n",
            "val Loss: 0.7291 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 87/399\n",
            "----------\n",
            "train Loss: 0.5728 Acc: 0.7404\n",
            "val Loss: 0.7320 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 88/399\n",
            "----------\n",
            "train Loss: 0.5753 Acc: 0.7494\n",
            "val Loss: 0.7160 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 89/399\n",
            "----------\n",
            "train Loss: 0.5626 Acc: 0.7514\n",
            "val Loss: 0.7327 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 90/399\n",
            "----------\n",
            "train Loss: 0.5665 Acc: 0.7522\n",
            "val Loss: 0.7248 Acc: 0.6796\n",
            "\n",
            "\n",
            "Epoch 91/399\n",
            "----------\n",
            "train Loss: 0.5850 Acc: 0.7435\n",
            "val Loss: 0.7233 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 92/399\n",
            "----------\n",
            "train Loss: 0.5765 Acc: 0.7450\n",
            "val Loss: 0.7346 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 93/399\n",
            "----------\n",
            "train Loss: 0.5677 Acc: 0.7458\n",
            "val Loss: 0.7214 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 94/399\n",
            "----------\n",
            "train Loss: 0.5839 Acc: 0.7453\n",
            "val Loss: 0.7342 Acc: 0.6694\n",
            "\n",
            "\n",
            "Epoch 95/399\n",
            "----------\n",
            "train Loss: 0.5634 Acc: 0.7432\n",
            "val Loss: 0.7312 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 96/399\n",
            "----------\n",
            "train Loss: 0.5745 Acc: 0.7404\n",
            "val Loss: 0.7415 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 97/399\n",
            "----------\n",
            "train Loss: 0.5697 Acc: 0.7506\n",
            "val Loss: 0.7367 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 98/399\n",
            "----------\n",
            "train Loss: 0.5780 Acc: 0.7425\n",
            "val Loss: 0.7310 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 99/399\n",
            "----------\n",
            "train Loss: 0.5757 Acc: 0.7486\n",
            "val Loss: 0.7336 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 100/399\n",
            "----------\n",
            "train Loss: 0.5861 Acc: 0.7414\n",
            "val Loss: 0.7144 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 101/399\n",
            "----------\n",
            "train Loss: 0.5746 Acc: 0.7491\n",
            "val Loss: 0.7274 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 102/399\n",
            "----------\n",
            "train Loss: 0.5656 Acc: 0.7506\n",
            "val Loss: 0.7250 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 103/399\n",
            "----------\n",
            "train Loss: 0.5821 Acc: 0.7524\n",
            "val Loss: 0.7303 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 104/399\n",
            "----------\n",
            "train Loss: 0.5625 Acc: 0.7519\n",
            "val Loss: 0.7074 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 105/399\n",
            "----------\n",
            "train Loss: 0.5676 Acc: 0.7440\n",
            "val Loss: 0.7233 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 106/399\n",
            "----------\n",
            "train Loss: 0.5813 Acc: 0.7392\n",
            "val Loss: 0.7424 Acc: 0.6643\n",
            "\n",
            "\n",
            "Epoch 107/399\n",
            "----------\n",
            "train Loss: 0.5636 Acc: 0.7402\n",
            "val Loss: 0.7271 Acc: 0.6704\n",
            "\n",
            "\n",
            "Epoch 108/399\n",
            "----------\n",
            "train Loss: 0.5612 Acc: 0.7501\n",
            "val Loss: 0.7220 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 109/399\n",
            "----------\n",
            "train Loss: 0.5745 Acc: 0.7425\n",
            "val Loss: 0.7333 Acc: 0.6714\n",
            "\n",
            "\n",
            "Epoch 110/399\n",
            "----------\n",
            "train Loss: 0.5512 Acc: 0.7583\n",
            "val Loss: 0.7270 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 111/399\n",
            "----------\n",
            "train Loss: 0.5624 Acc: 0.7435\n",
            "val Loss: 0.7231 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 112/399\n",
            "----------\n",
            "train Loss: 0.5379 Acc: 0.7688\n",
            "val Loss: 0.7230 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 113/399\n",
            "----------\n",
            "train Loss: 0.5695 Acc: 0.7491\n",
            "val Loss: 0.7150 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 114/399\n",
            "----------\n",
            "train Loss: 0.5622 Acc: 0.7552\n",
            "val Loss: 0.7366 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 115/399\n",
            "----------\n",
            "train Loss: 0.5689 Acc: 0.7496\n",
            "val Loss: 0.7307 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 116/399\n",
            "----------\n",
            "train Loss: 0.5690 Acc: 0.7458\n",
            "val Loss: 0.7258 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 117/399\n",
            "----------\n",
            "train Loss: 0.5662 Acc: 0.7437\n",
            "val Loss: 0.7341 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 118/399\n",
            "----------\n",
            "train Loss: 0.5699 Acc: 0.7573\n",
            "val Loss: 0.7156 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 119/399\n",
            "----------\n",
            "train Loss: 0.5722 Acc: 0.7496\n",
            "val Loss: 0.7367 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 120/399\n",
            "----------\n",
            "train Loss: 0.5863 Acc: 0.7412\n",
            "val Loss: 0.7322 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 121/399\n",
            "----------\n",
            "train Loss: 0.5720 Acc: 0.7414\n",
            "val Loss: 0.7185 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 122/399\n",
            "----------\n",
            "train Loss: 0.5807 Acc: 0.7363\n",
            "val Loss: 0.7314 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 123/399\n",
            "----------\n",
            "train Loss: 0.5662 Acc: 0.7478\n",
            "val Loss: 0.7199 Acc: 0.6745\n",
            "\n",
            "\n",
            "Epoch 124/399\n",
            "----------\n",
            "train Loss: 0.5626 Acc: 0.7568\n",
            "val Loss: 0.7234 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 125/399\n",
            "----------\n",
            "train Loss: 0.5767 Acc: 0.7529\n",
            "val Loss: 0.7323 Acc: 0.6704\n",
            "\n",
            "\n",
            "Epoch 126/399\n",
            "----------\n",
            "train Loss: 0.5685 Acc: 0.7448\n",
            "val Loss: 0.7396 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 127/399\n",
            "----------\n",
            "train Loss: 0.5621 Acc: 0.7573\n",
            "val Loss: 0.7217 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 128/399\n",
            "----------\n",
            "train Loss: 0.5763 Acc: 0.7537\n",
            "val Loss: 0.7263 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 129/399\n",
            "----------\n",
            "train Loss: 0.5693 Acc: 0.7468\n",
            "val Loss: 0.7303 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 130/399\n",
            "----------\n",
            "train Loss: 0.5728 Acc: 0.7506\n",
            "val Loss: 0.7494 Acc: 0.6551\n",
            "\n",
            "\n",
            "Epoch 131/399\n",
            "----------\n",
            "train Loss: 0.5421 Acc: 0.7621\n",
            "val Loss: 0.7236 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 132/399\n",
            "----------\n",
            "train Loss: 0.5506 Acc: 0.7565\n",
            "val Loss: 0.7539 Acc: 0.6796\n",
            "\n",
            "\n",
            "Epoch 133/399\n",
            "----------\n",
            "train Loss: 0.5550 Acc: 0.7578\n",
            "val Loss: 0.7245 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 134/399\n",
            "----------\n",
            "train Loss: 0.5635 Acc: 0.7534\n",
            "val Loss: 0.7338 Acc: 0.6735\n",
            "\n",
            "\n",
            "Epoch 135/399\n",
            "----------\n",
            "train Loss: 0.5508 Acc: 0.7527\n",
            "val Loss: 0.7251 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 136/399\n",
            "----------\n",
            "train Loss: 0.5685 Acc: 0.7527\n",
            "val Loss: 0.7373 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 137/399\n",
            "----------\n",
            "train Loss: 0.5701 Acc: 0.7376\n",
            "val Loss: 0.7300 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 138/399\n",
            "----------\n",
            "train Loss: 0.5551 Acc: 0.7575\n",
            "val Loss: 0.7305 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 139/399\n",
            "----------\n",
            "train Loss: 0.5619 Acc: 0.7540\n",
            "val Loss: 0.7444 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 140/399\n",
            "----------\n",
            "train Loss: 0.5534 Acc: 0.7568\n",
            "val Loss: 0.7520 Acc: 0.6714\n",
            "\n",
            "\n",
            "Epoch 141/399\n",
            "----------\n",
            "train Loss: 0.5708 Acc: 0.7481\n",
            "val Loss: 0.7401 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 142/399\n",
            "----------\n",
            "train Loss: 0.5673 Acc: 0.7389\n",
            "val Loss: 0.7279 Acc: 0.6990\n",
            "\n",
            "\n",
            "Epoch 143/399\n",
            "----------\n",
            "train Loss: 0.5658 Acc: 0.7481\n",
            "val Loss: 0.7271 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 144/399\n",
            "----------\n",
            "train Loss: 0.5641 Acc: 0.7425\n",
            "val Loss: 0.7367 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 145/399\n",
            "----------\n",
            "train Loss: 0.5534 Acc: 0.7547\n",
            "val Loss: 0.7368 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 146/399\n",
            "----------\n",
            "train Loss: 0.5628 Acc: 0.7489\n",
            "val Loss: 0.7322 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 147/399\n",
            "----------\n",
            "train Loss: 0.5457 Acc: 0.7593\n",
            "val Loss: 0.7560 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 148/399\n",
            "----------\n",
            "train Loss: 0.5727 Acc: 0.7443\n",
            "val Loss: 0.7280 Acc: 0.7051\n",
            "\n",
            "\n",
            "Epoch 149/399\n",
            "----------\n",
            "train Loss: 0.5627 Acc: 0.7540\n",
            "val Loss: 0.7402 Acc: 0.6745\n",
            "\n",
            "\n",
            "Epoch 150/399\n",
            "----------\n",
            "train Loss: 0.5802 Acc: 0.7369\n",
            "val Loss: 0.7421 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 151/399\n",
            "----------\n",
            "train Loss: 0.5604 Acc: 0.7560\n",
            "val Loss: 0.7347 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 152/399\n",
            "----------\n",
            "train Loss: 0.5574 Acc: 0.7486\n",
            "val Loss: 0.7245 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 153/399\n",
            "----------\n",
            "train Loss: 0.5755 Acc: 0.7440\n",
            "val Loss: 0.7303 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 154/399\n",
            "----------\n",
            "train Loss: 0.5636 Acc: 0.7494\n",
            "val Loss: 0.7241 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 155/399\n",
            "----------\n",
            "train Loss: 0.5663 Acc: 0.7483\n",
            "val Loss: 0.7405 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 156/399\n",
            "----------\n",
            "train Loss: 0.5585 Acc: 0.7524\n",
            "val Loss: 0.7348 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 157/399\n",
            "----------\n",
            "train Loss: 0.5622 Acc: 0.7522\n",
            "val Loss: 0.7230 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 158/399\n",
            "----------\n",
            "train Loss: 0.5574 Acc: 0.7611\n",
            "val Loss: 0.7365 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 159/399\n",
            "----------\n",
            "train Loss: 0.5613 Acc: 0.7489\n",
            "val Loss: 0.7183 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 160/399\n",
            "----------\n",
            "train Loss: 0.5686 Acc: 0.7407\n",
            "val Loss: 0.7317 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 161/399\n",
            "----------\n",
            "train Loss: 0.5894 Acc: 0.7340\n",
            "val Loss: 0.7365 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 162/399\n",
            "----------\n",
            "train Loss: 0.5631 Acc: 0.7545\n",
            "val Loss: 0.7260 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 163/399\n",
            "----------\n",
            "train Loss: 0.5472 Acc: 0.7662\n",
            "val Loss: 0.7189 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 164/399\n",
            "----------\n",
            "train Loss: 0.5615 Acc: 0.7517\n",
            "val Loss: 0.7455 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 165/399\n",
            "----------\n",
            "train Loss: 0.5727 Acc: 0.7483\n",
            "val Loss: 0.7203 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 166/399\n",
            "----------\n",
            "train Loss: 0.5602 Acc: 0.7463\n",
            "val Loss: 0.7371 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 167/399\n",
            "----------\n",
            "train Loss: 0.5557 Acc: 0.7545\n",
            "val Loss: 0.7352 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 168/399\n",
            "----------\n",
            "train Loss: 0.5681 Acc: 0.7537\n",
            "val Loss: 0.7271 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 169/399\n",
            "----------\n",
            "train Loss: 0.5619 Acc: 0.7519\n",
            "val Loss: 0.7314 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 170/399\n",
            "----------\n",
            "train Loss: 0.5563 Acc: 0.7499\n",
            "val Loss: 0.7338 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 171/399\n",
            "----------\n",
            "train Loss: 0.5483 Acc: 0.7621\n",
            "val Loss: 0.7294 Acc: 0.7010\n",
            "\n",
            "\n",
            "Epoch 172/399\n",
            "----------\n",
            "train Loss: 0.5595 Acc: 0.7608\n",
            "val Loss: 0.7226 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 173/399\n",
            "----------\n",
            "train Loss: 0.5605 Acc: 0.7483\n",
            "val Loss: 0.7185 Acc: 0.7010\n",
            "\n",
            "\n",
            "Epoch 174/399\n",
            "----------\n",
            "train Loss: 0.5695 Acc: 0.7575\n",
            "val Loss: 0.7318 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 175/399\n",
            "----------\n",
            "train Loss: 0.5574 Acc: 0.7545\n",
            "val Loss: 0.7176 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 176/399\n",
            "----------\n",
            "train Loss: 0.5576 Acc: 0.7522\n",
            "val Loss: 0.7227 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 177/399\n",
            "----------\n",
            "train Loss: 0.5760 Acc: 0.7440\n",
            "val Loss: 0.7399 Acc: 0.7061\n",
            "\n",
            "\n",
            "Epoch 178/399\n",
            "----------\n",
            "train Loss: 0.5443 Acc: 0.7524\n",
            "val Loss: 0.7334 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 179/399\n",
            "----------\n",
            "train Loss: 0.5454 Acc: 0.7504\n",
            "val Loss: 0.7304 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 180/399\n",
            "----------\n",
            "train Loss: 0.5437 Acc: 0.7580\n",
            "val Loss: 0.7343 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 181/399\n",
            "----------\n",
            "train Loss: 0.5551 Acc: 0.7509\n",
            "val Loss: 0.7331 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 182/399\n",
            "----------\n",
            "train Loss: 0.5728 Acc: 0.7414\n",
            "val Loss: 0.7270 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 183/399\n",
            "----------\n",
            "train Loss: 0.5547 Acc: 0.7634\n",
            "val Loss: 0.7245 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 184/399\n",
            "----------\n",
            "train Loss: 0.5680 Acc: 0.7463\n",
            "val Loss: 0.7181 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 185/399\n",
            "----------\n",
            "train Loss: 0.5649 Acc: 0.7580\n",
            "val Loss: 0.7231 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 186/399\n",
            "----------\n",
            "train Loss: 0.5596 Acc: 0.7481\n",
            "val Loss: 0.7144 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 187/399\n",
            "----------\n",
            "train Loss: 0.5450 Acc: 0.7629\n",
            "val Loss: 0.7201 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 188/399\n",
            "----------\n",
            "train Loss: 0.5478 Acc: 0.7598\n",
            "val Loss: 0.7134 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 189/399\n",
            "----------\n",
            "train Loss: 0.5569 Acc: 0.7563\n",
            "val Loss: 0.7121 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 190/399\n",
            "----------\n",
            "train Loss: 0.5552 Acc: 0.7532\n",
            "val Loss: 0.7278 Acc: 0.6796\n",
            "\n",
            "\n",
            "Epoch 191/399\n",
            "----------\n",
            "train Loss: 0.5745 Acc: 0.7437\n",
            "val Loss: 0.7425 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 192/399\n",
            "----------\n",
            "train Loss: 0.5537 Acc: 0.7621\n",
            "val Loss: 0.7193 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 193/399\n",
            "----------\n",
            "train Loss: 0.5671 Acc: 0.7499\n",
            "val Loss: 0.7186 Acc: 0.7102\n",
            "\n",
            "\n",
            "Epoch 194/399\n",
            "----------\n",
            "train Loss: 0.5479 Acc: 0.7563\n",
            "val Loss: 0.7176 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 195/399\n",
            "----------\n",
            "train Loss: 0.5518 Acc: 0.7573\n",
            "val Loss: 0.7202 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 196/399\n",
            "----------\n",
            "train Loss: 0.5527 Acc: 0.7563\n",
            "val Loss: 0.7257 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 197/399\n",
            "----------\n",
            "train Loss: 0.5645 Acc: 0.7504\n",
            "val Loss: 0.7168 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 198/399\n",
            "----------\n",
            "train Loss: 0.5597 Acc: 0.7453\n",
            "val Loss: 0.7284 Acc: 0.6724\n",
            "\n",
            "\n",
            "Epoch 199/399\n",
            "----------\n",
            "train Loss: 0.5665 Acc: 0.7519\n",
            "val Loss: 0.7294 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 200/399\n",
            "----------\n",
            "train Loss: 0.5596 Acc: 0.7506\n",
            "val Loss: 0.7160 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 201/399\n",
            "----------\n",
            "train Loss: 0.5457 Acc: 0.7563\n",
            "val Loss: 0.7208 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 202/399\n",
            "----------\n",
            "train Loss: 0.5541 Acc: 0.7486\n",
            "val Loss: 0.7264 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 203/399\n",
            "----------\n",
            "train Loss: 0.5397 Acc: 0.7639\n",
            "val Loss: 0.7297 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 204/399\n",
            "----------\n",
            "train Loss: 0.5353 Acc: 0.7662\n",
            "val Loss: 0.7261 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 205/399\n",
            "----------\n",
            "train Loss: 0.5573 Acc: 0.7511\n",
            "val Loss: 0.7136 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 206/399\n",
            "----------\n",
            "train Loss: 0.5631 Acc: 0.7552\n",
            "val Loss: 0.7211 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 207/399\n",
            "----------\n",
            "train Loss: 0.5510 Acc: 0.7591\n",
            "val Loss: 0.7215 Acc: 0.6684\n",
            "\n",
            "\n",
            "Epoch 208/399\n",
            "----------\n",
            "train Loss: 0.5695 Acc: 0.7417\n",
            "val Loss: 0.7222 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 209/399\n",
            "----------\n",
            "train Loss: 0.5497 Acc: 0.7611\n",
            "val Loss: 0.7088 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 210/399\n",
            "----------\n",
            "train Loss: 0.5447 Acc: 0.7657\n",
            "val Loss: 0.7127 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 211/399\n",
            "----------\n",
            "train Loss: 0.5516 Acc: 0.7578\n",
            "val Loss: 0.7281 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 212/399\n",
            "----------\n",
            "train Loss: 0.5299 Acc: 0.7593\n",
            "val Loss: 0.7178 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 213/399\n",
            "----------\n",
            "train Loss: 0.5728 Acc: 0.7409\n",
            "val Loss: 0.7172 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 214/399\n",
            "----------\n",
            "train Loss: 0.5487 Acc: 0.7511\n",
            "val Loss: 0.7133 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 215/399\n",
            "----------\n",
            "train Loss: 0.5625 Acc: 0.7463\n",
            "val Loss: 0.7240 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 216/399\n",
            "----------\n",
            "train Loss: 0.5424 Acc: 0.7619\n",
            "val Loss: 0.7094 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 217/399\n",
            "----------\n",
            "train Loss: 0.5548 Acc: 0.7478\n",
            "val Loss: 0.7156 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 218/399\n",
            "----------\n",
            "train Loss: 0.5374 Acc: 0.7660\n",
            "val Loss: 0.7078 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 219/399\n",
            "----------\n",
            "train Loss: 0.5515 Acc: 0.7565\n",
            "val Loss: 0.7110 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 220/399\n",
            "----------\n",
            "train Loss: 0.5533 Acc: 0.7593\n",
            "val Loss: 0.7186 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 221/399\n",
            "----------\n",
            "train Loss: 0.5294 Acc: 0.7662\n",
            "val Loss: 0.7138 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 222/399\n",
            "----------\n",
            "train Loss: 0.5386 Acc: 0.7611\n",
            "val Loss: 0.7363 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 223/399\n",
            "----------\n",
            "train Loss: 0.5561 Acc: 0.7563\n",
            "val Loss: 0.7160 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 224/399\n",
            "----------\n",
            "train Loss: 0.5403 Acc: 0.7578\n",
            "val Loss: 0.7287 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 225/399\n",
            "----------\n",
            "train Loss: 0.5505 Acc: 0.7555\n",
            "val Loss: 0.7207 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 226/399\n",
            "----------\n",
            "train Loss: 0.5443 Acc: 0.7631\n",
            "val Loss: 0.7070 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 227/399\n",
            "----------\n",
            "train Loss: 0.5670 Acc: 0.7450\n",
            "val Loss: 0.7256 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 228/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7586\n",
            "val Loss: 0.7311 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 229/399\n",
            "----------\n",
            "train Loss: 0.5666 Acc: 0.7483\n",
            "val Loss: 0.7069 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 230/399\n",
            "----------\n",
            "train Loss: 0.5344 Acc: 0.7652\n",
            "val Loss: 0.7126 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 231/399\n",
            "----------\n",
            "train Loss: 0.5551 Acc: 0.7565\n",
            "val Loss: 0.7184 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 232/399\n",
            "----------\n",
            "train Loss: 0.5433 Acc: 0.7575\n",
            "val Loss: 0.7233 Acc: 0.6990\n",
            "\n",
            "\n",
            "Epoch 233/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7578\n",
            "val Loss: 0.7352 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 234/399\n",
            "----------\n",
            "train Loss: 0.5565 Acc: 0.7547\n",
            "val Loss: 0.7218 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 235/399\n",
            "----------\n",
            "train Loss: 0.5452 Acc: 0.7591\n",
            "val Loss: 0.7200 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 236/399\n",
            "----------\n",
            "train Loss: 0.5695 Acc: 0.7440\n",
            "val Loss: 0.7089 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 237/399\n",
            "----------\n",
            "train Loss: 0.5485 Acc: 0.7603\n",
            "val Loss: 0.7149 Acc: 0.6724\n",
            "\n",
            "\n",
            "Epoch 238/399\n",
            "----------\n",
            "train Loss: 0.5461 Acc: 0.7621\n",
            "val Loss: 0.7121 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 239/399\n",
            "----------\n",
            "train Loss: 0.5442 Acc: 0.7593\n",
            "val Loss: 0.7201 Acc: 0.7031\n",
            "\n",
            "\n",
            "Epoch 240/399\n",
            "----------\n",
            "train Loss: 0.5705 Acc: 0.7440\n",
            "val Loss: 0.7301 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 241/399\n",
            "----------\n",
            "train Loss: 0.5562 Acc: 0.7601\n",
            "val Loss: 0.7264 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 242/399\n",
            "----------\n",
            "train Loss: 0.5365 Acc: 0.7647\n",
            "val Loss: 0.7305 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 243/399\n",
            "----------\n",
            "train Loss: 0.5517 Acc: 0.7629\n",
            "val Loss: 0.7225 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 244/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7534\n",
            "val Loss: 0.7307 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 245/399\n",
            "----------\n",
            "train Loss: 0.5390 Acc: 0.7662\n",
            "val Loss: 0.7242 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 246/399\n",
            "----------\n",
            "train Loss: 0.5494 Acc: 0.7596\n",
            "val Loss: 0.7198 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 247/399\n",
            "----------\n",
            "train Loss: 0.5401 Acc: 0.7667\n",
            "val Loss: 0.7194 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 248/399\n",
            "----------\n",
            "train Loss: 0.5514 Acc: 0.7545\n",
            "val Loss: 0.7203 Acc: 0.6796\n",
            "\n",
            "\n",
            "Epoch 249/399\n",
            "----------\n",
            "train Loss: 0.5448 Acc: 0.7639\n",
            "val Loss: 0.7280 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 250/399\n",
            "----------\n",
            "train Loss: 0.5442 Acc: 0.7688\n",
            "val Loss: 0.7378 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 251/399\n",
            "----------\n",
            "train Loss: 0.5516 Acc: 0.7601\n",
            "val Loss: 0.7263 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 252/399\n",
            "----------\n",
            "train Loss: 0.5289 Acc: 0.7693\n",
            "val Loss: 0.7254 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 253/399\n",
            "----------\n",
            "train Loss: 0.5393 Acc: 0.7644\n",
            "val Loss: 0.7296 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 254/399\n",
            "----------\n",
            "train Loss: 0.5434 Acc: 0.7644\n",
            "val Loss: 0.7143 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 255/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7688\n",
            "val Loss: 0.7388 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 256/399\n",
            "----------\n",
            "train Loss: 0.5341 Acc: 0.7639\n",
            "val Loss: 0.7329 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 257/399\n",
            "----------\n",
            "train Loss: 0.5292 Acc: 0.7690\n",
            "val Loss: 0.7401 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 258/399\n",
            "----------\n",
            "train Loss: 0.5502 Acc: 0.7540\n",
            "val Loss: 0.7316 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 259/399\n",
            "----------\n",
            "train Loss: 0.5525 Acc: 0.7611\n",
            "val Loss: 0.7359 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 260/399\n",
            "----------\n",
            "train Loss: 0.5500 Acc: 0.7614\n",
            "val Loss: 0.7230 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 261/399\n",
            "----------\n",
            "train Loss: 0.5341 Acc: 0.7698\n",
            "val Loss: 0.7331 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 262/399\n",
            "----------\n",
            "train Loss: 0.5443 Acc: 0.7542\n",
            "val Loss: 0.7442 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 263/399\n",
            "----------\n",
            "train Loss: 0.5391 Acc: 0.7621\n",
            "val Loss: 0.7200 Acc: 0.6724\n",
            "\n",
            "\n",
            "Epoch 264/399\n",
            "----------\n",
            "train Loss: 0.5381 Acc: 0.7703\n",
            "val Loss: 0.7388 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 265/399\n",
            "----------\n",
            "train Loss: 0.5495 Acc: 0.7624\n",
            "val Loss: 0.7432 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 266/399\n",
            "----------\n",
            "train Loss: 0.5432 Acc: 0.7598\n",
            "val Loss: 0.7279 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 267/399\n",
            "----------\n",
            "train Loss: 0.5563 Acc: 0.7555\n",
            "val Loss: 0.7353 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 268/399\n",
            "----------\n",
            "train Loss: 0.5481 Acc: 0.7606\n",
            "val Loss: 0.7459 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 269/399\n",
            "----------\n",
            "train Loss: 0.5359 Acc: 0.7575\n",
            "val Loss: 0.7274 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 270/399\n",
            "----------\n",
            "train Loss: 0.5530 Acc: 0.7552\n",
            "val Loss: 0.7296 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 271/399\n",
            "----------\n",
            "train Loss: 0.5483 Acc: 0.7580\n",
            "val Loss: 0.7293 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 272/399\n",
            "----------\n",
            "train Loss: 0.5467 Acc: 0.7639\n",
            "val Loss: 0.7090 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 273/399\n",
            "----------\n",
            "train Loss: 0.5403 Acc: 0.7647\n",
            "val Loss: 0.7363 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 274/399\n",
            "----------\n",
            "train Loss: 0.5352 Acc: 0.7657\n",
            "val Loss: 0.7378 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 275/399\n",
            "----------\n",
            "train Loss: 0.5510 Acc: 0.7586\n",
            "val Loss: 0.7339 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 276/399\n",
            "----------\n",
            "train Loss: 0.5566 Acc: 0.7506\n",
            "val Loss: 0.7168 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 277/399\n",
            "----------\n",
            "train Loss: 0.5513 Acc: 0.7529\n",
            "val Loss: 0.7265 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 278/399\n",
            "----------\n",
            "train Loss: 0.5499 Acc: 0.7583\n",
            "val Loss: 0.7248 Acc: 0.6990\n",
            "\n",
            "\n",
            "Epoch 279/399\n",
            "----------\n",
            "train Loss: 0.5324 Acc: 0.7603\n",
            "val Loss: 0.7059 Acc: 0.7031\n",
            "\n",
            "\n",
            "Epoch 280/399\n",
            "----------\n",
            "train Loss: 0.5523 Acc: 0.7565\n",
            "val Loss: 0.7289 Acc: 0.7010\n",
            "\n",
            "\n",
            "Epoch 281/399\n",
            "----------\n",
            "train Loss: 0.5470 Acc: 0.7583\n",
            "val Loss: 0.7337 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 282/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7700\n",
            "val Loss: 0.7258 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 283/399\n",
            "----------\n",
            "train Loss: 0.5532 Acc: 0.7504\n",
            "val Loss: 0.7213 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 284/399\n",
            "----------\n",
            "train Loss: 0.5396 Acc: 0.7578\n",
            "val Loss: 0.7230 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 285/399\n",
            "----------\n",
            "train Loss: 0.5483 Acc: 0.7675\n",
            "val Loss: 0.7139 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 286/399\n",
            "----------\n",
            "train Loss: 0.5527 Acc: 0.7660\n",
            "val Loss: 0.7334 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 287/399\n",
            "----------\n",
            "train Loss: 0.5363 Acc: 0.7644\n",
            "val Loss: 0.7208 Acc: 0.7031\n",
            "\n",
            "\n",
            "Epoch 288/399\n",
            "----------\n",
            "train Loss: 0.5459 Acc: 0.7667\n",
            "val Loss: 0.7313 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 289/399\n",
            "----------\n",
            "train Loss: 0.5387 Acc: 0.7608\n",
            "val Loss: 0.7348 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 290/399\n",
            "----------\n",
            "train Loss: 0.5654 Acc: 0.7542\n",
            "val Loss: 0.7545 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 291/399\n",
            "----------\n",
            "train Loss: 0.5524 Acc: 0.7624\n",
            "val Loss: 0.7256 Acc: 0.7031\n",
            "\n",
            "\n",
            "Epoch 292/399\n",
            "----------\n",
            "train Loss: 0.5481 Acc: 0.7619\n",
            "val Loss: 0.7339 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 293/399\n",
            "----------\n",
            "train Loss: 0.5348 Acc: 0.7662\n",
            "val Loss: 0.7281 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 294/399\n",
            "----------\n",
            "train Loss: 0.5494 Acc: 0.7570\n",
            "val Loss: 0.7437 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 295/399\n",
            "----------\n",
            "train Loss: 0.5437 Acc: 0.7601\n",
            "val Loss: 0.7258 Acc: 0.7010\n",
            "\n",
            "\n",
            "Epoch 296/399\n",
            "----------\n",
            "train Loss: 0.5488 Acc: 0.7547\n",
            "val Loss: 0.7242 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 297/399\n",
            "----------\n",
            "train Loss: 0.5349 Acc: 0.7700\n",
            "val Loss: 0.7551 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 298/399\n",
            "----------\n",
            "train Loss: 0.5418 Acc: 0.7601\n",
            "val Loss: 0.7363 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 299/399\n",
            "----------\n",
            "train Loss: 0.5369 Acc: 0.7596\n",
            "val Loss: 0.7394 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 300/399\n",
            "----------\n",
            "train Loss: 0.5583 Acc: 0.7504\n",
            "val Loss: 0.7355 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 301/399\n",
            "----------\n",
            "train Loss: 0.5405 Acc: 0.7608\n",
            "val Loss: 0.7294 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 302/399\n",
            "----------\n",
            "train Loss: 0.5463 Acc: 0.7626\n",
            "val Loss: 0.7440 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 303/399\n",
            "----------\n",
            "train Loss: 0.5437 Acc: 0.7583\n",
            "val Loss: 0.7192 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 304/399\n",
            "----------\n",
            "train Loss: 0.5327 Acc: 0.7777\n",
            "val Loss: 0.7308 Acc: 0.7122\n",
            "\n",
            "\n",
            "Epoch 305/399\n",
            "----------\n",
            "train Loss: 0.5541 Acc: 0.7524\n",
            "val Loss: 0.7229 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 306/399\n",
            "----------\n",
            "train Loss: 0.5452 Acc: 0.7524\n",
            "val Loss: 0.7295 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 307/399\n",
            "----------\n",
            "train Loss: 0.5494 Acc: 0.7642\n",
            "val Loss: 0.7144 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 308/399\n",
            "----------\n",
            "train Loss: 0.5299 Acc: 0.7634\n",
            "val Loss: 0.7306 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 309/399\n",
            "----------\n",
            "train Loss: 0.5402 Acc: 0.7657\n",
            "val Loss: 0.7277 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 310/399\n",
            "----------\n",
            "train Loss: 0.5437 Acc: 0.7680\n",
            "val Loss: 0.7375 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 311/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7634\n",
            "val Loss: 0.7243 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 312/399\n",
            "----------\n",
            "train Loss: 0.5575 Acc: 0.7583\n",
            "val Loss: 0.7509 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 313/399\n",
            "----------\n",
            "train Loss: 0.5227 Acc: 0.7746\n",
            "val Loss: 0.7183 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 314/399\n",
            "----------\n",
            "train Loss: 0.5430 Acc: 0.7642\n",
            "val Loss: 0.7288 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 315/399\n",
            "----------\n",
            "train Loss: 0.5492 Acc: 0.7552\n",
            "val Loss: 0.7443 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 316/399\n",
            "----------\n",
            "train Loss: 0.5458 Acc: 0.7578\n",
            "val Loss: 0.7138 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 317/399\n",
            "----------\n",
            "train Loss: 0.5372 Acc: 0.7624\n",
            "val Loss: 0.7356 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 318/399\n",
            "----------\n",
            "train Loss: 0.5357 Acc: 0.7654\n",
            "val Loss: 0.7405 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 319/399\n",
            "----------\n",
            "train Loss: 0.5632 Acc: 0.7509\n",
            "val Loss: 0.7139 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 320/399\n",
            "----------\n",
            "train Loss: 0.5581 Acc: 0.7486\n",
            "val Loss: 0.7151 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 321/399\n",
            "----------\n",
            "train Loss: 0.5320 Acc: 0.7652\n",
            "val Loss: 0.7389 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 322/399\n",
            "----------\n",
            "train Loss: 0.5476 Acc: 0.7580\n",
            "val Loss: 0.7231 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 323/399\n",
            "----------\n",
            "train Loss: 0.5468 Acc: 0.7557\n",
            "val Loss: 0.7347 Acc: 0.6990\n",
            "\n",
            "\n",
            "Epoch 324/399\n",
            "----------\n",
            "train Loss: 0.5403 Acc: 0.7578\n",
            "val Loss: 0.7137 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 325/399\n",
            "----------\n",
            "train Loss: 0.5331 Acc: 0.7675\n",
            "val Loss: 0.7229 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 326/399\n",
            "----------\n",
            "train Loss: 0.5292 Acc: 0.7644\n",
            "val Loss: 0.7378 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 327/399\n",
            "----------\n",
            "train Loss: 0.5464 Acc: 0.7547\n",
            "val Loss: 0.7242 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 328/399\n",
            "----------\n",
            "train Loss: 0.5594 Acc: 0.7532\n",
            "val Loss: 0.7409 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 329/399\n",
            "----------\n",
            "train Loss: 0.5394 Acc: 0.7688\n",
            "val Loss: 0.7199 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 330/399\n",
            "----------\n",
            "train Loss: 0.5416 Acc: 0.7596\n",
            "val Loss: 0.7377 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 331/399\n",
            "----------\n",
            "train Loss: 0.5475 Acc: 0.7631\n",
            "val Loss: 0.7245 Acc: 0.6776\n",
            "\n",
            "\n",
            "Epoch 332/399\n",
            "----------\n",
            "train Loss: 0.5415 Acc: 0.7626\n",
            "val Loss: 0.7082 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 333/399\n",
            "----------\n",
            "train Loss: 0.5372 Acc: 0.7634\n",
            "val Loss: 0.7437 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 334/399\n",
            "----------\n",
            "train Loss: 0.5556 Acc: 0.7540\n",
            "val Loss: 0.7257 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 335/399\n",
            "----------\n",
            "train Loss: 0.5279 Acc: 0.7736\n",
            "val Loss: 0.7297 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 336/399\n",
            "----------\n",
            "train Loss: 0.5507 Acc: 0.7527\n",
            "val Loss: 0.7341 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 337/399\n",
            "----------\n",
            "train Loss: 0.5628 Acc: 0.7437\n",
            "val Loss: 0.7400 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 338/399\n",
            "----------\n",
            "train Loss: 0.5320 Acc: 0.7695\n",
            "val Loss: 0.7377 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 339/399\n",
            "----------\n",
            "train Loss: 0.5400 Acc: 0.7665\n",
            "val Loss: 0.7360 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 340/399\n",
            "----------\n",
            "train Loss: 0.5403 Acc: 0.7672\n",
            "val Loss: 0.7335 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 341/399\n",
            "----------\n",
            "train Loss: 0.5364 Acc: 0.7665\n",
            "val Loss: 0.7175 Acc: 0.7051\n",
            "\n",
            "\n",
            "Epoch 342/399\n",
            "----------\n",
            "train Loss: 0.5421 Acc: 0.7608\n",
            "val Loss: 0.7209 Acc: 0.7031\n",
            "\n",
            "\n",
            "Epoch 343/399\n",
            "----------\n",
            "train Loss: 0.5382 Acc: 0.7598\n",
            "val Loss: 0.7151 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 344/399\n",
            "----------\n",
            "train Loss: 0.5589 Acc: 0.7654\n",
            "val Loss: 0.7177 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 345/399\n",
            "----------\n",
            "train Loss: 0.5470 Acc: 0.7644\n",
            "val Loss: 0.7338 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 346/399\n",
            "----------\n",
            "train Loss: 0.5433 Acc: 0.7723\n",
            "val Loss: 0.7203 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 347/399\n",
            "----------\n",
            "train Loss: 0.5448 Acc: 0.7639\n",
            "val Loss: 0.7272 Acc: 0.6939\n",
            "\n",
            "\n",
            "Epoch 348/399\n",
            "----------\n",
            "train Loss: 0.5313 Acc: 0.7644\n",
            "val Loss: 0.7149 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 349/399\n",
            "----------\n",
            "train Loss: 0.5373 Acc: 0.7652\n",
            "val Loss: 0.7232 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 350/399\n",
            "----------\n",
            "train Loss: 0.5348 Acc: 0.7708\n",
            "val Loss: 0.7355 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 351/399\n",
            "----------\n",
            "train Loss: 0.5481 Acc: 0.7591\n",
            "val Loss: 0.7151 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 352/399\n",
            "----------\n",
            "train Loss: 0.5368 Acc: 0.7695\n",
            "val Loss: 0.7245 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 353/399\n",
            "----------\n",
            "train Loss: 0.5534 Acc: 0.7565\n",
            "val Loss: 0.7203 Acc: 0.7071\n",
            "\n",
            "\n",
            "Epoch 354/399\n",
            "----------\n",
            "train Loss: 0.5387 Acc: 0.7693\n",
            "val Loss: 0.7378 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 355/399\n",
            "----------\n",
            "train Loss: 0.5385 Acc: 0.7703\n",
            "val Loss: 0.7313 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 356/399\n",
            "----------\n",
            "train Loss: 0.5384 Acc: 0.7657\n",
            "val Loss: 0.7178 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 357/399\n",
            "----------\n",
            "train Loss: 0.5431 Acc: 0.7570\n",
            "val Loss: 0.7296 Acc: 0.6918\n",
            "\n",
            "\n",
            "Epoch 358/399\n",
            "----------\n",
            "train Loss: 0.5350 Acc: 0.7542\n",
            "val Loss: 0.7213 Acc: 0.7102\n",
            "\n",
            "\n",
            "Epoch 359/399\n",
            "----------\n",
            "train Loss: 0.5509 Acc: 0.7578\n",
            "val Loss: 0.7087 Acc: 0.6837\n",
            "\n",
            "\n",
            "Epoch 360/399\n",
            "----------\n",
            "train Loss: 0.5417 Acc: 0.7619\n",
            "val Loss: 0.7081 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 361/399\n",
            "----------\n",
            "train Loss: 0.5396 Acc: 0.7568\n",
            "val Loss: 0.7218 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 362/399\n",
            "----------\n",
            "train Loss: 0.5521 Acc: 0.7540\n",
            "val Loss: 0.7284 Acc: 0.7051\n",
            "\n",
            "\n",
            "Epoch 363/399\n",
            "----------\n",
            "train Loss: 0.5340 Acc: 0.7598\n",
            "val Loss: 0.7302 Acc: 0.6990\n",
            "\n",
            "\n",
            "Epoch 364/399\n",
            "----------\n",
            "train Loss: 0.5414 Acc: 0.7649\n",
            "val Loss: 0.7364 Acc: 0.6990\n",
            "\n",
            "\n",
            "Epoch 365/399\n",
            "----------\n",
            "train Loss: 0.5289 Acc: 0.7698\n",
            "val Loss: 0.7147 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 366/399\n",
            "----------\n",
            "train Loss: 0.5445 Acc: 0.7578\n",
            "val Loss: 0.7073 Acc: 0.7041\n",
            "\n",
            "\n",
            "Epoch 367/399\n",
            "----------\n",
            "train Loss: 0.5357 Acc: 0.7578\n",
            "val Loss: 0.7378 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 368/399\n",
            "----------\n",
            "train Loss: 0.5566 Acc: 0.7524\n",
            "val Loss: 0.7402 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 369/399\n",
            "----------\n",
            "train Loss: 0.5378 Acc: 0.7705\n",
            "val Loss: 0.7311 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 370/399\n",
            "----------\n",
            "train Loss: 0.5408 Acc: 0.7591\n",
            "val Loss: 0.7219 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 371/399\n",
            "----------\n",
            "train Loss: 0.5496 Acc: 0.7529\n",
            "val Loss: 0.7297 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 372/399\n",
            "----------\n",
            "train Loss: 0.5321 Acc: 0.7682\n",
            "val Loss: 0.7295 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 373/399\n",
            "----------\n",
            "train Loss: 0.5352 Acc: 0.7728\n",
            "val Loss: 0.7144 Acc: 0.7020\n",
            "\n",
            "\n",
            "Epoch 374/399\n",
            "----------\n",
            "train Loss: 0.5399 Acc: 0.7631\n",
            "val Loss: 0.7332 Acc: 0.6786\n",
            "\n",
            "\n",
            "Epoch 375/399\n",
            "----------\n",
            "train Loss: 0.5371 Acc: 0.7649\n",
            "val Loss: 0.7304 Acc: 0.6816\n",
            "\n",
            "\n",
            "Epoch 376/399\n",
            "----------\n",
            "train Loss: 0.5282 Acc: 0.7700\n",
            "val Loss: 0.7350 Acc: 0.6857\n",
            "\n",
            "\n",
            "Epoch 377/399\n",
            "----------\n",
            "train Loss: 0.5367 Acc: 0.7560\n",
            "val Loss: 0.7317 Acc: 0.6806\n",
            "\n",
            "\n",
            "Epoch 378/399\n",
            "----------\n",
            "train Loss: 0.5494 Acc: 0.7649\n",
            "val Loss: 0.7346 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 379/399\n",
            "----------\n",
            "train Loss: 0.5368 Acc: 0.7698\n",
            "val Loss: 0.7305 Acc: 0.6765\n",
            "\n",
            "\n",
            "Epoch 380/399\n",
            "----------\n",
            "train Loss: 0.5289 Acc: 0.7626\n",
            "val Loss: 0.7314 Acc: 0.6847\n",
            "\n",
            "\n",
            "Epoch 381/399\n",
            "----------\n",
            "train Loss: 0.5272 Acc: 0.7649\n",
            "val Loss: 0.7261 Acc: 0.6959\n",
            "\n",
            "\n",
            "Epoch 382/399\n",
            "----------\n",
            "train Loss: 0.5269 Acc: 0.7616\n",
            "val Loss: 0.7421 Acc: 0.6867\n",
            "\n",
            "\n",
            "Epoch 383/399\n",
            "----------\n",
            "train Loss: 0.5416 Acc: 0.7601\n",
            "val Loss: 0.7253 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 384/399\n",
            "----------\n",
            "train Loss: 0.5335 Acc: 0.7616\n",
            "val Loss: 0.7231 Acc: 0.6878\n",
            "\n",
            "\n",
            "Epoch 385/399\n",
            "----------\n",
            "train Loss: 0.5312 Acc: 0.7662\n",
            "val Loss: 0.7375 Acc: 0.6888\n",
            "\n",
            "\n",
            "Epoch 386/399\n",
            "----------\n",
            "train Loss: 0.5367 Acc: 0.7593\n",
            "val Loss: 0.7323 Acc: 0.6827\n",
            "\n",
            "\n",
            "Epoch 387/399\n",
            "----------\n",
            "train Loss: 0.5430 Acc: 0.7670\n",
            "val Loss: 0.7253 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 388/399\n",
            "----------\n",
            "train Loss: 0.5309 Acc: 0.7728\n",
            "val Loss: 0.7244 Acc: 0.6969\n",
            "\n",
            "\n",
            "Epoch 389/399\n",
            "----------\n",
            "train Loss: 0.5486 Acc: 0.7662\n",
            "val Loss: 0.7161 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 390/399\n",
            "----------\n",
            "train Loss: 0.5405 Acc: 0.7631\n",
            "val Loss: 0.7261 Acc: 0.6980\n",
            "\n",
            "\n",
            "Epoch 391/399\n",
            "----------\n",
            "train Loss: 0.5251 Acc: 0.7672\n",
            "val Loss: 0.7398 Acc: 0.7010\n",
            "\n",
            "\n",
            "Epoch 392/399\n",
            "----------\n",
            "train Loss: 0.5480 Acc: 0.7568\n",
            "val Loss: 0.7384 Acc: 0.6898\n",
            "\n",
            "\n",
            "Epoch 393/399\n",
            "----------\n",
            "train Loss: 0.5305 Acc: 0.7731\n",
            "val Loss: 0.7348 Acc: 0.6929\n",
            "\n",
            "\n",
            "Epoch 394/399\n",
            "----------\n",
            "train Loss: 0.5369 Acc: 0.7670\n",
            "val Loss: 0.7302 Acc: 0.6908\n",
            "\n",
            "\n",
            "Epoch 395/399\n",
            "----------\n",
            "train Loss: 0.5296 Acc: 0.7688\n",
            "val Loss: 0.7349 Acc: 0.7000\n",
            "\n",
            "\n",
            "Epoch 396/399\n",
            "----------\n",
            "train Loss: 0.5150 Acc: 0.7685\n",
            "val Loss: 0.7221 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 397/399\n",
            "----------\n",
            "train Loss: 0.5445 Acc: 0.7649\n",
            "val Loss: 0.7359 Acc: 0.6949\n",
            "\n",
            "\n",
            "Epoch 398/399\n",
            "----------\n",
            "train Loss: 0.5479 Acc: 0.7532\n",
            "val Loss: 0.7437 Acc: 0.6755\n",
            "\n",
            "\n",
            "Epoch 399/399\n",
            "----------\n",
            "train Loss: 0.5431 Acc: 0.7603\n",
            "val Loss: 0.7306 Acc: 0.6888\n",
            "\n",
            "\n",
            "Training complete in 13m 35s\n",
            "Best val Acc: 0.712245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXf6kfrCqAXy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}